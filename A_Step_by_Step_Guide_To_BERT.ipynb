{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPW0QPRVMMHmK5liRSW++bL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Spartanlasergun/A-Step-by-Step-Guide-to-BERT/blob/main/A_Step_by_Step_Guide_To_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Step by Step Guide to BERT\n",
        "##### by Spartanlasergun\n",
        "-----\n",
        "\n",
        "The layout of this guide is very simple. It is meant to be read from top to bottom - i.e. step by step. It's also better if you work directly in google collaboratory rather than reading the notebook from github.\n",
        "\n",
        "The theoretical understanding that is necessary for using BERT is given in markdown cells that typically appear before or after the relevant blocks of code. To simplify things even further, this guide uses external modules sparingly - and these external modules are only imported in the cells where they are used to avoid confusing the reader concerning which module is in use for a specific task. I have also tried to avoid using dataframes in favour of basic python lists/arrays as most people may find it much more comfortable to stick with the basics.\n",
        "\n",
        "This guide assumes that you have some basic knowledge of programming and statistics, though it is my hope that even without this, most of the necessary understanding will still be imparted.\n",
        "\n",
        "-----\n",
        "\n",
        "# A Brief Introduction\n",
        "\n",
        "What is machine learning really? What are we doing here? What are we trying to accomplish when we say that we wish to **classify** a bit of text? Well, it's no different than classifying something in real life. We live in world of many things - apples, oranges, trees, flowers, birds, fishes etc. None of these are the same. When we talk about classifying text, we want to understand something specific about the nature of the text. Does it say something positive or negative? Is it gramatically correct? Is it poetry, a narrative essay, a speech, or perhaps something else?\n",
        "\n",
        "It's easy enough to read and understand a piece of text as a human being, but if we were to give this text as input to a computer, how then would it understand - how then would it be able to **classify**? The answer is simple - the machine must learn.\n",
        "\n",
        "In the case of BERT, this means taking large archives of text data from places like wikipedia and identifying meaningful representations of words, phrases and sentences that can be used to produce meaningful mathematical representations of any given set of words, phrases or sentences. This mathematical **model** can the be used to determine if a given set of text data is more similar to text that is happy, sad, angry, a poem, a tweet, musical lyrics etc. Theoretically, there are no limits to the categories we can assign.\n",
        "\n",
        "So how do we take a word and represent it mathematically? Or, how do we take a sentence and do the same?\n",
        "\n",
        "Cosider the word **'The'**. Lets assign a number to this word.\n",
        "\n",
        "Let the word **'The'** be represented by the integer **1234**.\n",
        "\n",
        "Now that we've done this, there's nothing to stop us from assigning unique numbers to all of the words in the english language. In fact, lets take things a step further. Consider the sentence:\n",
        "\n",
        "**The sky is blue.**\n",
        "\n",
        "Assuming that we have some dictionary containing unique integer values for each word/symbol in the english language, we can assign unique numbers to each word/symbol in this sentence such that:\n",
        "\n",
        "**The --> 1234**\n",
        "\n",
        "**sky --> 435**\n",
        "\n",
        "**is --> 2389**\n",
        "\n",
        "**blue --> 678**\n",
        "\n",
        "**. --> 390**\n",
        "\n",
        "Now, we have technically represented this sentence mathematically with the list of numbers:\n",
        "\n",
        "$$[1234, 435, 2389, 678, 390]$$\n",
        "\n",
        "This is good, but its not yet quite enough. Truthfully, we have very barely captured anything at all - in fact, all we've done is convert the sentence into a list of numbers. To actually capture the meaning in a sentence means considering the actual content of each word, the positions of each word in relation to all the others, the context of the sentence itself and all of the words that constitute it, along with many other factors.\n",
        "\n",
        "There is no natural law that guides the process of capturing this meaning, but rather it is the culmination of years of rigorous testing that has resulted in the production of models like **BERT** that can do the job for us. As we shall soon see, our simple list of numbers will become transformed into a very complex vector that is typically hundreds of units long - but very accurate as a mathematical representation of a sentence.\n",
        "\n",
        "The vectors that **BERT** produces are typically of the length 768 or 1024, but there are smaller models ranging from 128 to 512 units. Since we want to conserve processing we will be using one of the smaller models. You can picture the vectors that BERT produces as a list of long rational numbers that have no discernible meaning at a glance, i.e.\n",
        "\n",
        "$$[3.3432532, -4.56462222, 7.4535535345, -4.54654646, ...]$$\n",
        "\n",
        "For now, there is little that we need to understand about how these vectors are produced mathematically. In this introductory guide, I simply want to show you how to load some text data, prepare it for processing with **BERT**, generate the vectors that represent our text, and use these vectors to do some simple text classification."
      ],
      "metadata": {
        "id": "l8QvYWvzyMoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "# Loading Some Text Data\n",
        "\n",
        "We begin with the simple task of loading some text data. Since spreadsheet files (.csv, .tsv, etc.) are most typically used in the field for sharing machine learning data, I have decided that it is best to load data from this type of file. To do this we use the pandas module, which has built in functions for automatically reading spreadsheet data from local and online repsotories.\n",
        "\n",
        "(Note: There is no reason why we couldn't simply read data from a regular text file, or even create a python list manually with some data that we want to test. But it's still important to know how to work with spreadsheet files.)\n",
        "\n",
        "In the code below, I import the pandas module, define the url (download-link) for my spreadsheet, and I use the 'pd.read_csv' function to read the data from my spreasheet into a pandas dataframe."
      ],
      "metadata": {
        "id": "PyBnxfxv2jy7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "GLDiq5jpyJ-T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81207146-c108-43dc-f18f-8f9c54e28112"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7920, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = 'https://github.com/Spartanlasergun/A-Step-by-Step-Guide-to-BERT/raw/main/data.csv'\n",
        "\n",
        "dataframe = pd.read_csv(url)\n",
        "dataframe.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the '.shape' function we can see the shape of the dataset that we have loaded. In total it contains 7920 datapoints, and 3 different columns of data. We can print a small sample of this data using the '.sample' function as is shown below."
      ],
      "metadata": {
        "id": "6tkb25Xi8UQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.sample(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "ZDLRbXI80RmW",
        "outputId": "c5c02219-67c1-4027-8608-571ee5a18c79"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id  label                                              tweet\n",
              "1719  1720      0  Birthday present! :)) #apple. #iphone. #5. #wh...\n",
              "5853  5854      0  Yes!!! Totally unexpected ! Thank you ninong V...\n",
              "5086  5087      0  I dont care.. #instadaily #moment #stranger #a..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-63082322-b680-4f51-9234-26423f543575\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1719</th>\n",
              "      <td>1720</td>\n",
              "      <td>0</td>\n",
              "      <td>Birthday present! :)) #apple. #iphone. #5. #wh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5853</th>\n",
              "      <td>5854</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes!!! Totally unexpected ! Thank you ninong V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5086</th>\n",
              "      <td>5087</td>\n",
              "      <td>0</td>\n",
              "      <td>I dont care.. #instadaily #moment #stranger #a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-63082322-b680-4f51-9234-26423f543575')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-63082322-b680-4f51-9234-26423f543575 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-63082322-b680-4f51-9234-26423f543575');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-688ba6ba-36f2-4d06-8dd7-f1e1b8642a7b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-688ba6ba-36f2-4d06-8dd7-f1e1b8642a7b')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-688ba6ba-36f2-4d06-8dd7-f1e1b8642a7b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the above dataset contains 3 columns - id, label, and tweet. The id represents the row number, which we are not interested in. The tweet column contains tweets that have either positive sentiment, or negative sentiment. The labels correspond to these tweets, with a value of $0$ representing a positive tweet, while a value of $1$ represents a negative tweet. We are going to use these tweets and their labels to train a model that will be able to classify any given tweet as either positive of negative.\n",
        "\n",
        "First we must extract the data that we want. The tweets are extracted in the cell below."
      ],
      "metadata": {
        "id": "jpZKxXgx8z1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting the tweets from our data frame\n",
        "extract_tweets = dataframe.tweet.values # here we extract our tweets from the dataframe\n",
        "tweets = extract_tweets.tolist() # here we convert our dataframe to a regular python list\n",
        "print(tweets[0])\n",
        "print(tweets[1])\n",
        "print(tweets[2])"
      ],
      "metadata": {
        "id": "eEgw8d5e2CiO",
        "outputId": "3cc911f2-186b-4886-d153-87626c9afb6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#fingerprint #Pregnancy Test https://goo.gl/h1MfQV #android #apps #beautiful #cute #health #igers #iphoneonly #iphonesia #iphone\n",
            "Finally a transparant silicon case ^^ Thanks to my uncle :) #yay #Sony #Xperia #S #sonyexperias… http://instagram.com/p/YGEt5JC6JM/\n",
            "We love this! Would you go? #talk #makememories #unplug #relax #iphone #smartphone #wifi #connect... http://fb.me/6N3LsUpCu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, I have also taken the liberty of converting our dataframe back into a regular python list. I have also printed the first few tweets.\n",
        "\n",
        "Below we extract the labels, and convert it to a regular python list in the same fashion."
      ],
      "metadata": {
        "id": "GQi2oLzgBiRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting the labels from our data frame\n",
        "extract_labels = dataframe.label.values # here we extract our labels from the dataframe\n",
        "labels = extract_labels.tolist() # here we convert our dataframe to a regular python list\n",
        "print(labels[0])\n",
        "print(labels[1])\n",
        "print(labels[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNW13CqLCDTs",
        "outputId": "06cb305d-615d-45ab-fd89-8aae81dcede8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, the labels have been extracted and we have a regular python list that we can work with. I have also printed the first few labels. These would correspond to the first few tweets that we printed above. Since the numbers are all $0$ this means that they all have positive sentiment."
      ],
      "metadata": {
        "id": "HJU5c-aICaLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "# Preparing our text data for BERT\n",
        "\n",
        "Before we can give our text data to BERT, there is a bit of additional preparation that must be done. In order for BERT to effectively recognize the start and end of our tweets (or any text data), we must include some special data.\n",
        "\n",
        "Lets consider the sentence:\n",
        "\n",
        "**The sky is blue.**\n",
        "\n",
        "For BERT to recognize the beginning and end of this sentence we must add the characters **[CLS]** and **[SEP]** where we wish to designate the start and end of the sentence. Thus, our original sentence must become:\n",
        "\n",
        "**[CLS] The sky is blue. [SEP]**\n",
        "\n",
        "In the case of our tweets, we must add the **[CLS]** and **[SEP]** tokens in the same manner to the start and end of each. This is done in the code below."
      ],
      "metadata": {
        "id": "fgEodldODp-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding CLS and SEP tokens at the beginning and end of each sentence for BERT\n",
        "bertified_tweets = []\n",
        "for text in tweets:\n",
        "    bertify = \"[CLS] \" + text + \" [SEP]\"\n",
        "    bertified_tweets.append(bertify)\n",
        "\n",
        "print(bertified_tweets[0])\n",
        "print(bertified_tweets[1])\n",
        "print(bertified_tweets[2])"
      ],
      "metadata": {
        "id": "gx5aIBaS2JSP",
        "outputId": "047b7035-d531-4087-b023-59d8971d7f81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] #fingerprint #Pregnancy Test https://goo.gl/h1MfQV #android #apps #beautiful #cute #health #igers #iphoneonly #iphonesia #iphone [SEP]\n",
            "[CLS] Finally a transparant silicon case ^^ Thanks to my uncle :) #yay #Sony #Xperia #S #sonyexperias… http://instagram.com/p/YGEt5JC6JM/ [SEP]\n",
            "[CLS] We love this! Would you go? #talk #makememories #unplug #relax #iphone #smartphone #wifi #connect... http://fb.me/6N3LsUpCu [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see from the output above, we have added the necessary tokens to the start and end of each tweet."
      ],
      "metadata": {
        "id": "LBhbMg_qPxZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "# Installing transformers\n",
        "\n",
        "In this section we install the transformers module. I have not gone into any kind of examination of transfomers themselves as this is not necessary for our simple guide. Many textbooks insist on giving at least an overview of transformers, but I have have found that it only overcomplicates the process without giving much insight into machine learning itself. What you need to understand, is that BERT is a transformer itself and the many layers of processing that is done by the BERT transformer to generate our vector representation of text is not readily accesible to the average user. I leave it to the user to explore the larger treatise on the issue and to come to his/her own conclusions or understanding of what necessarily needs to be understood on that point.\n",
        "\n",
        "The transformers module that we will install is maintained as an open sourced effort by HuggingFace, of which the BERT model is just one of many transformers and machine learning models that you can access freely."
      ],
      "metadata": {
        "id": "w9vJzaGeQCka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "g2spmI5n2gvC",
        "outputId": "56d6691d-f4c1-4b80-f026-acee7f1f25d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "# Tokenizing our input\n",
        "\n",
        "What is tokenization? Tokenization is the process of breaking down our text into individual sentences or words. For instance, if I have the sentences:\n",
        "\n",
        "[\"The sky is blue. The trees are green.\"]\n",
        "\n",
        "The tokenized **sentence** would be:\n",
        "\n",
        "[[\"The sky is blue.\"], [\"The trees are green.\"]]\n",
        "\n",
        "A step further and the tokenized **words** would be:\n",
        "\n",
        "[[\"The\", \"sky\", \"is\", \"blue\", \".\"], [\"The\", \"trees\", \"are\", \"green\", \".\"]]\n",
        "\n",
        "Essentially we want to take our list of tweets and break it down into an easily accesible list of words that we can use. This is easy enough to do manually, but if we are working with a list of thousands of sentences - in this case, tweets - then we need some way of automatically processing our data to obtain our list of words. The mechanism that we use to do this, is called a 'tokenizer'. There are many tokenizer's available, and some may break down sentences differently than others. Since we are using **BERT** we will use the **BERT** tokenizer.\n",
        "\n",
        "In the cell below, we import the BertTokenizer and use it to break down each individual tweet in our dataset into a list of words."
      ],
      "metadata": {
        "id": "6BilQpyEJxcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Activating the BERT Tokenizer\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# create an instance of the BERT tokenzation class\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# run our list of 'BERT-approved' sentences through the tokenizer\n",
        "tokenized_texts = []\n",
        "for tweet in bertified_tweets:\n",
        "    tokenized = tokenizer.tokenize(tweet)\n",
        "    tokenized_texts.append(tokenized)\n",
        "\n",
        "print(tokenized_texts[0])\n",
        "print(tokenized_texts[1])\n",
        "print(tokenized_texts[2])"
      ],
      "metadata": {
        "id": "Hu6ovZ1R2QdT",
        "outputId": "ec82924f-1869-4fb1-a352-121846db0ceb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', '#', 'finger', '##print', '#', 'pregnancy', 'test', 'https', ':', '/', '/', 'goo', '.', 'g', '##l', '/', 'h', '##1', '##m', '##f', '##q', '##v', '#', 'android', '#', 'apps', '#', 'beautiful', '#', 'cute', '#', 'health', '#', 'i', '##gers', '#', 'iphone', '##on', '##ly', '#', 'iphone', '##sia', '#', 'iphone', '[SEP]']\n",
            "['[CLS]', 'finally', 'a', 'trans', '##para', '##nt', 'silicon', 'case', '^', '^', 'thanks', 'to', 'my', 'uncle', ':', ')', '#', 'ya', '##y', '#', 'sony', '#', 'xp', '##eria', '#', 's', '#', 'sony', '##ex', '##per', '##ias', '…', 'http', ':', '/', '/', 'ins', '##tagram', '.', 'com', '/', 'p', '/', 'y', '##get', '##5', '##j', '##c', '##6', '##jm', '/', '[SEP]']\n",
            "['[CLS]', 'we', 'love', 'this', '!', 'would', 'you', 'go', '?', '#', 'talk', '#', 'make', '##me', '##mori', '##es', '#', 'un', '##pl', '##ug', '#', 'relax', '#', 'iphone', '#', 'smartphone', '#', 'wi', '##fi', '#', 'connect', '.', '.', '.', 'http', ':', '/', '/', 'f', '##b', '.', 'me', '/', '6', '##n', '##3', '##ls', '##up', '##cu', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see from the output, our list of tweets have now been broken down into individual words.\n",
        "\n",
        "This is only part of our tokenization process. As discussed in the introduction, what we are trying to do is to mathematically represent the meaning in each of these tweets. This requires that we convert our list of words/symbols into a list of integer values. Each word/symbol will have a unqiue integer equivalent that will be assigned to it.\n",
        "\n",
        "To do this, we will use feed our list of words (formerly list of tweets) into our tokenizer and use the convert to ids function.\n",
        "\n",
        "This is done below."
      ],
      "metadata": {
        "id": "XV5qT8YENXno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = []\n",
        "\n",
        "for x in tokenized_texts:\n",
        "    create_id = tokenizer.convert_tokens_to_ids(x)\n",
        "    token_ids.append(create_id)\n",
        "\n",
        "print(token_ids[0])\n",
        "print(token_ids[1])\n",
        "print(token_ids[2])"
      ],
      "metadata": {
        "id": "T5Blo77r2nzu",
        "outputId": "c5f87e7c-8191-4f43-a36d-08c761f424ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 1001, 4344, 16550, 1001, 10032, 3231, 16770, 1024, 1013, 1013, 27571, 1012, 1043, 2140, 1013, 1044, 2487, 2213, 2546, 4160, 2615, 1001, 11924, 1001, 18726, 1001, 3376, 1001, 10140, 1001, 2740, 1001, 1045, 15776, 1001, 18059, 2239, 2135, 1001, 18059, 8464, 1001, 18059, 102]\n",
            "[101, 2633, 1037, 9099, 28689, 3372, 13773, 2553, 1034, 1034, 4283, 2000, 2026, 4470, 1024, 1007, 1001, 8038, 2100, 1001, 8412, 1001, 26726, 11610, 1001, 1055, 1001, 8412, 10288, 4842, 7951, 1529, 8299, 1024, 1013, 1013, 16021, 23091, 1012, 4012, 1013, 1052, 1013, 1061, 18150, 2629, 3501, 2278, 2575, 24703, 1013, 102]\n",
            "[101, 2057, 2293, 2023, 999, 2052, 2017, 2175, 1029, 1001, 2831, 1001, 2191, 4168, 24610, 2229, 1001, 4895, 24759, 15916, 1001, 9483, 1001, 18059, 1001, 26381, 1001, 15536, 8873, 1001, 7532, 1012, 1012, 1012, 8299, 1024, 1013, 1013, 1042, 2497, 1012, 2033, 1013, 1020, 2078, 2509, 4877, 6279, 10841, 102]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take note of the output. Our list of tokenized texts are now a list ok token ids - i.e. a list of numbers."
      ],
      "metadata": {
        "id": "EcdPfT9hRiCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "# Padding the token ids\n",
        "\n",
        "In order for the BERT algorithm to function properly, each of our list of token ids must be the same length. We refer to this property as the sequence length. To ensure that each list of token ids we give to the BERT model has the same sequence length we must add padding to the sentences that are shorter in our dataset. This padding is simply the number $0$.\n",
        "\n",
        "We can determine which is the longest list of token ids in our dataset and hence, determine an appropriate sequence length based on this value."
      ],
      "metadata": {
        "id": "HLuJRA5aRyzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 0\n",
        "for item in token_ids:\n",
        "  if sequence_length <= len(item):\n",
        "    sequence_length = len(item)\n",
        "\n",
        "print(sequence_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFZAjjw-TOjj",
        "outputId": "302789e1-6363-44b5-b543-83ff82da1989"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above output, we can see that the longest tweet in our dataset constitutes 142 separate words. Hence, we need to pad all of our shorter lists with the number $0$ until they are all of the same length - 142.\n",
        "\n",
        "(The sequence length may be typically be extended beyond the length of the longest sentence in the dataset, but should not be less.)\n",
        "\n",
        "In the code below, we add a bunch of zeroes to the lists in our token_ids variable that are less than 142 units in length, until they meet the required value."
      ],
      "metadata": {
        "id": "nKkaCpvWTpTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = []\n",
        "\n",
        "for item in token_ids:\n",
        "  count = len(item)\n",
        "  temp = item\n",
        "  while count != sequence_length:\n",
        "    temp.append(0)\n",
        "    count = count + 1\n",
        "  input_ids.append(temp)\n",
        "\n",
        "print(input_ids[0])\n",
        "print(len(input_ids[0]))\n",
        "print(input_ids[1])\n",
        "print(len(input_ids[1]))"
      ],
      "metadata": {
        "id": "jhv-2DRt2zaw",
        "outputId": "ac6b65b3-56a6-47bc-f459-46335637ea80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 1001, 4344, 16550, 1001, 10032, 3231, 16770, 1024, 1013, 1013, 27571, 1012, 1043, 2140, 1013, 1044, 2487, 2213, 2546, 4160, 2615, 1001, 11924, 1001, 18726, 1001, 3376, 1001, 10140, 1001, 2740, 1001, 1045, 15776, 1001, 18059, 2239, 2135, 1001, 18059, 8464, 1001, 18059, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "142\n",
            "[101, 2633, 1037, 9099, 28689, 3372, 13773, 2553, 1034, 1034, 4283, 2000, 2026, 4470, 1024, 1007, 1001, 8038, 2100, 1001, 8412, 1001, 26726, 11610, 1001, 1055, 1001, 8412, 10288, 4842, 7951, 1529, 8299, 1024, 1013, 1013, 16021, 23091, 1012, 4012, 1013, 1052, 1013, 1061, 18150, 2629, 3501, 2278, 2575, 24703, 1013, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take note of our output. All of our token_ids lists have been padded with 0's to meet the required sequence length of 142."
      ],
      "metadata": {
        "id": "BRN4zVrTUxe7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "# Attention Masks - our final bit of pre-processing\n",
        "\n",
        "Things get a bit complex here in ways that may feel overwhelming, but I assure you it is very simple.\n",
        "\n",
        "In addition to our input_ids (now of the fixed length 142), BERT also requires something called an attention mask.\n",
        "\n",
        "This mask is a list of 1's and 0's corresponding to our input_ids, where the number 1 represents a word that BERT will operate on, while the number 0 represents a word that is 'masked' or hidden from the BERT machine.\n",
        "\n",
        "We do not want BERT to operate on the padded values that we have added to our input ids, so we need to create a list analgous to our input that contains 1's for each real word in the sentence and 0's for each of the padded values. Since the padding itself is given by the number 0, the attention masks are easy to create. This is done below:\n"
      ],
      "metadata": {
        "id": "ZCWZr3MAYX7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for item in input_ids:\n",
        "    temp = []\n",
        "    for number in item:\n",
        "      if number != 0:\n",
        "        temp.append(1)\n",
        "      else:\n",
        "        temp.append(0)\n",
        "    attention_masks.append(temp)\n",
        "\n",
        "print(attention_masks[0])"
      ],
      "metadata": {
        "id": "3mDjpkuj3-n5",
        "outputId": "5c4e0480-543e-4c01-9094-06485662cda8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take note of the output. As described, our attention masks are simply lists of 1's and 0's where 1 represents a real token/word in our input, and 0 represents a padded value that we wish to ignore.\n",
        "\n",
        "And with this last step we're finished with the pre-processing. We have our input_ids and now we have our attention_masks. In the next section we look at initializing the BERT model."
      ],
      "metadata": {
        "id": "CXtthnBqgQEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "# Feeding the data into BERT\n",
        "\n",
        "In creating this guide, I have explored numerous different methods and techniques that each constitute their own path to understanding text classification with BERT. In wanting to maintain simplicity for the sake of the user, I have selected what I have thought are the simplest methods and explanations that could be given, and yet I have not found a way around using a **dataloader** for feeding information into BERT. Thus, I am forced to take you through the following complexities, but there is a silver lining. In exploring the use of dataloaders, I think we can come to a real hands on understanding and appreciation for the massive amounts of resources that machine learning typically requires.\n",
        "\n",
        "We begin with a very simple task. BERT uses a particular data type - the 'torch tensor'. So we must convert our 'input_ids' and 'attention_masks' into valid torch tensors that we can give to the BERT model. This is done in the cell below."
      ],
      "metadata": {
        "id": "oZSJdp7iJMKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "bert_input = torch.tensor(input_ids)\n",
        "bert_mask = torch.tensor(attention_masks)"
      ],
      "metadata": {
        "id": "PEpZodmm4Iks"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we must import and initialize the BERT model. In this case, we wish to conserve as much processing as possible, so we will be using the smallest BERT model available - the BERT-tiny model. This model produces vectors of the size 128, while the others are 256, 512, 768 and 1024. Using the tiny model means that our text classification will be less accurate, but this is fine for our simple guide. However, if you wish to experiment with some of the larger models you can simply replace the model name with:\n",
        "\n",
        "'google/bert_uncased_L-2_H-256_A-2' ---> for the 256 BERT model\n",
        "\n",
        "'google/bert_uncased_L-4_H-512_A-8' ---> for the 512 BERT model\n",
        "\n",
        "'google/bert_uncased_L-12_H-768_A-12' ---> for the 768 BERT model"
      ],
      "metadata": {
        "id": "d-mWxitKUlrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel  # import the BERT model from transformers\n",
        "\n",
        "model_name = 'google/bert_uncased_L-2_H-128_A-2' # define the name of the Tiny BERT model\n",
        "model = BertModel.from_pretrained(model_name)    # instantiate the BERT model"
      ],
      "metadata": {
        "id": "bCPBmmJI4V_e"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahhh! I have my BERT model ready to go. In my ideal workflow, I would simply like to give the BERT model the torch tensors (input_ids and attention_masks) and let it produce the vectors that I require, but this is not possible. BERT is a bit of a greedy old dog. He won't take our data without trying to allocate memory for the entire operation and this will overload our meagre bit of RAM. So we're forced to feed him information bit by bit. In computing terms, we are going to process the data, batch by batch.\n",
        "\n",
        "But before we do this, the user can expermient a bit. The code below contains the ideal workflow as I have described to you.\n",
        "\n",
        "```\n",
        "outputs = model(input_ids=bert_input, attention_mask=bert_mask)\n",
        "```\n",
        "\n",
        "If you run the code, it should crash the google colab. You do not necessarily have to run it, but if you do, remember that you will need to reload all of the cells from the beginning.\n",
        "\n",
        "(I have found that if you reduce the size of the dataset to around 100 tweets instead of the ~8000 that we are using, it would just barely be able to run. But for machine learning purposes, datasets in the hundreds of thousands are the norm and this makes the dataloader very important)\n",
        "\n",
        "The first step in using our dataloader is to create a combined dataset of our input_ids and attention_masks. Recall that we just converted these into torch tensors. In the cell below, these tensors are combined into one."
      ],
      "metadata": {
        "id": "6mxC3itcXlgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "dataset = TensorDataset(bert_input, bert_mask)"
      ],
      "metadata": {
        "id": "DIOqmXKA23Ve"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a combined dataset we can go ahead and create the dataloader. As we talked about, this dataloader is going to allow us to feed data into BERT into smaller more managable batches.\n",
        "\n",
        "In the cell below, the DataLoader module is imported, and we define the dataloader based on our combined dataset and a specific batch size. The batch size we are using is 8, which means that BERT will only process 8 of our inputs at any given point in time. This will prevent it from trying to allocate memory for our entire dataset and crashing the operation."
      ],
      "metadata": {
        "id": "04iHN662BSb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=8)"
      ],
      "metadata": {
        "id": "aVdSvhkC3k2S"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our dataloader, we can begin to generate the vectors for our dataset.\n",
        "\n",
        "(My griping over having to forcefully explain the dataloader is because of the cell below. This cannot be split into smaller parts. It is simply the operation that we must do.)\n",
        "\n",
        "I begin by defining an empty array that we can push our vectors into as BERT processes each batch, i.e. - 'tweet_vectors'\n",
        "\n",
        "The dataloader that we created can be iterated through like a regular python list. Since it contains the amalgamation of our input_ids and attention_masks, the zeroth index in each batch will have 8 of our input_ids, and the first index will have 8 of the corresponding attention_masks.\n",
        "\n",
        "(We use the torch.no_grad() function to generate our output because I have experienced random crashes without it. This function specifies that the algorithm should not do gradient calculations and so reduces the intensity of the processing required.)\n",
        "\n",
        "Once the outputs are generated, we retrieve the vectors that represent each individual tweet and push them into our tweet_vectors array.\n",
        "\n",
        "On this point the reader should take note, that BERT produces vectors for each individual tweet and then uses the aggregate of the vectors to represent the entire tweet/sentence itself. The vectors are accessible at both the word level and the sentence level, but since we are simply trying to classify tweets as positive or negative, we need only take the vectors that represent our entire tweet/sentence. In additon to this, BERT takes our input through 13 different layers of processing. The vectors generated at each layer is also accessible to the user, but this is far more advanced than most people may ever need to know. The user can explore these vectors at his/her own discretion."
      ],
      "metadata": {
        "id": "k4YihGAWCF6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create lists to store the model outputs\n",
        "tweet_vectors = []\n",
        "\n",
        "# Iterate through the data loader and store model outputs\n",
        "for batch in dataloader:\n",
        "  input = batch[0]   # get 8 input ids\n",
        "  masks = batch[1]   # get 8 corresponding attention masks\n",
        "  with torch.no_grad():   # using torch.no_grad()\n",
        "    outputs = model(input_ids=input, attention_mask=masks)   # generate the vectors for our batch of 8\n",
        "\n",
        "  # retrieve the vectors and append to our list\n",
        "  tweet_aggregate = outputs.pooler_output     # retrieve the sentence aggregate vectors\n",
        "\n",
        "  for tweet_vec in tweet_aggregate:           # append all 8 vectors to our empty list\n",
        "    tweet_vectors.append((tweet_vec.tolist()))"
      ],
      "metadata": {
        "id": "fpI8hIdg4cMQ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the 'outputs.pooler_output' function gives us a tuple with all of the 8 vectors contained in any given batch. Since I want my vectors in a regular one dimensional list, I iterate through and append each one individually instead of appending the list itself.\n",
        "\n",
        "To obtain the word level vectors you would use:\n",
        "\n",
        "```\n",
        "word_vec = outputs.last_hidden_state\n",
        "```\n",
        "But as stated before, this is not necessary for our classifier. Many other guides will walk you through the syntax associated with retrieving these vectors as well as the vectors produced by each layer of BERT.\n",
        "\n",
        "For now, we have now sucessfully obtained our vectors in the 'tweet_vectors' variable - which is all that we are concerned with.\n",
        "\n",
        "Take note that the length of the tweet_vectors match the size of our orginal dataset, which means that we do indeed have a vector that represents each tweet."
      ],
      "metadata": {
        "id": "uu9rziLfT2vL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tweet_vectors))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iM7C436gIxd6",
        "outputId": "109eb1ec-1610-41de-9709-c2ce4f5b74ed"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7920\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also note, that each vector is of the size 128 - as is produced by the BERT tiny model"
      ],
      "metadata": {
        "id": "rr3weE8OWgYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tweet_vectors[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OYDQY8EVtVu",
        "outputId": "493599cf-1152-41ef-f8b4-12f1f83e38f5"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "# Classification using Logistic Regression\n",
        "\n",
        "For a simple tutorial, it makes no sense to attempt to explain logistic regression, as it is an advanced mathematical concept. This means that the user will have to take a lot of what happens in this section for granted.\n",
        "\n",
        "To perform classification, we first have to train a model based on the vectors that we have obtained for our dataset. This means many things. But first, I want to address the phrase **'train a model'**. What does this mean? It means that we are going to take the text that we have accurately represented mathematically - our tweet_vectors - and use it to define a mathematical function that can represent all text data of this type. Since our text data represents tweets that are positive or negative, our mathematical function will specifically represent this subset of meaning in the english language. In this way, we can predict whether or not a given tweet is in fact **positive** or **negative** using the mathematical function that we have defined.\n",
        "\n",
        "In order to ensure that my mathematical function is accurately classifying tweets as positive or negative, we need to be able to test it with a set already classified data so that we can understand it's degree of accuracy. You may recall, that the data that we are working with is already classified/labelled with 1's and 0's to indicate positive and negative sentiment. We can use the majority of this data to train the model, and leave a couple hundred datapoints for testing the model afterward. In total we have 7920 datapoints, and in the cell below I extract the first 7000 tweet vectors and their corresponding labels to be used as training data. The remaining 920 will be used for testing the accuracy model at the end."
      ],
      "metadata": {
        "id": "5JAU45m1iUUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_vectors = tweet_vectors[0:7000]\n",
        "train_labels = labels[0:7000]"
      ],
      "metadata": {
        "id": "LYkGmHOZi5lt"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that I have my training data, I can go ahead and begin to train the model. In this case, we are using a logisitc regression model. The user will have to take this for granted, as this is a very advanced bit of mathematics that cannot be easily explained in the context of this tutorial. Fortunately, it is not necessary to understand it as we have python modules that can easily do the job for us. (If you wish to explore the math, I would reccommend using the book: Logistic Regression: A Self Learning Text, by David Kleinbaum and Mitchel Klein).\n",
        "\n",
        "In the cell below, I import the LogisticRegression model from Sklearn library and I initialize the logistic regression classifier. I then give the training data to the model - this includes the tweet vectors and labels that we have previously allocated for training."
      ],
      "metadata": {
        "id": "PgPsnRWhAHQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize a Logistic Regression classifier\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "classifier.fit(training_vectors, train_labels)"
      ],
      "metadata": {
        "id": "O98ktxTgiflV",
        "outputId": "1189ce8d-1818-4946-b6a7-11ab285e2c5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(max_iter=1000)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And there you have it! The model has been trained. Now we need to test the model to ensure that it is working properly.\n",
        "\n",
        "In the cell below, I extract the remaining 920 tweet vectors and labels that we have reserved for testing our model."
      ],
      "metadata": {
        "id": "PTLMZaUcBog0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testing_vectors = tweet_vectors[7000:7920]\n",
        "test_labels = labels[7000:7920]"
      ],
      "metadata": {
        "id": "Ss17-KYPkOAW"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! we have our testing data. Before we can use it with our classifier, we have one extra step.\n",
        "\n",
        "The Sklearn Logistic Classifier requires that our input data be a two dimensional array, or it will not function properly. This means that I simply need to take the data from our one dimensional array and append it to an empty list so that the Sklearn Classifier will have its 2d array.\n",
        "\n",
        "This is done in the cell below."
      ],
      "metadata": {
        "id": "X6tPOu7GCBUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t_vec = []\n",
        "for item in testing_vectors:\n",
        "    temp = [item]\n",
        "    t_vec.append(temp)"
      ],
      "metadata": {
        "id": "pY1C3zPkkz_t"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can begin to make our predictions. Recall that we have two labels - 0 representing a positive tweet, and 1 representing a negative tweet. Hence, our classifier is going to predict that the given tweet is either positive - 0 - or negaitve - 1.\n",
        "\n",
        "I want to store the predictions in a list so that I can compare the predicted values with the predetermined labels. This will allow me to determine what percentage of the tweets were accurately classified by the Logistic Model.\n",
        "\n",
        "In the cell below, I define an empty list to store the predictions. I iterate through the list of tweet vectors that have been reserved for testing and feed them into the model one by one to generate predictions for each."
      ],
      "metadata": {
        "id": "mrzl_l23C0dU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "for test_vec in t_vec:\n",
        "    # Make predictions using the trained Logistic Regression classifier\n",
        "    predicted_class = classifier.predict(test_vec)\n",
        "\n",
        "    # store the predictions in its own array\n",
        "    predictions.append(int(predicted_class))\n",
        "\n",
        "print(predictions)\n",
        "print(test_labels)"
      ],
      "metadata": {
        "id": "Kq63wXoYkiCm",
        "outputId": "cd4a1c5a-66ea-4e13-f4cb-082f2165b3c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take note of the output. I have printed the list of predictions, and directly below it is a list of the predetermined labels that represent the correct answer/prediction. For each value that corresponds exactly, our model has made an accurate prediction. At a glance, you will notice that not all the values are accurate. We need to check to see what percentage it has actually been able to classify accurately. This is done in the cell below."
      ],
      "metadata": {
        "id": "PDHrkaxIEJek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = 0\n",
        "correct = 0\n",
        "for prophecy in predictions:\n",
        "    if prophecy == test_labels[index]:\n",
        "        correct = correct + 1\n",
        "    index = index + 1\n",
        "\n",
        "accuracy = (correct / len(predictions)) * 100\n",
        "print(\"The classifier is \" + str(accuracy) + \"% accurate\")"
      ],
      "metadata": {
        "id": "-CHwApZik9wN",
        "outputId": "ef50cdcd-9732-4dbe-9c6b-5197dd92f69b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The classifier is 88.26086956521739% accurate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classifier is roughly 88% accurate - that's actually very impressive considering that we are using the smallest BERT model. The larger models wil probably give more than 90% on average."
      ],
      "metadata": {
        "id": "7etng7e5E5wU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "# Having some fun with our sentiment classifier\n",
        "\n",
        "Now that we have trained a model that can accurately classify tweets as positive or negative, we can play around with this by making up some tweets of our own an seeing how well it works.\n",
        "\n",
        "In the cell below, I define some tweets that the user can alter to his/her own desire. In this case, I've made sure that the first one is negative, and the second one is positive."
      ],
      "metadata": {
        "id": "-N_h7wG8PLUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweety_bird = [\"Elon Musk is a complete asshole #thenewtwittersucks\", \"Elon Musk is a genius! X is much better than twitter <3 #X\"]"
      ],
      "metadata": {
        "id": "KMka10_DP193"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can take the tweets defined above and process it for classification using the same code that we used to create the model. Since this is just a rehash of the code that we have used for the tutorial, I won't bother discussing it, and we can just go ahead and run the cell. (I have literally just copied and pasted most of the blocks of code that we have already discussed above.)"
      ],
      "metadata": {
        "id": "-_5dqmIzQCJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding CLS and SEP tokens at the beginning and end of each sentence for BERT\n",
        "bertified_tweets = []\n",
        "for text in tweety_bird:\n",
        "    bertify = \"[CLS] \" + text + \" [SEP]\"\n",
        "    bertified_tweets.append(bertify)\n",
        "\n",
        "# run our list of 'BERT-approved' sentences through the tokenizer\n",
        "tokenized_texts = []\n",
        "for tweet in bertified_tweets:\n",
        "    tokenized = tokenizer.tokenize(tweet)\n",
        "    tokenized_texts.append(tokenized)\n",
        "\n",
        "token_ids = []\n",
        "\n",
        "for x in tokenized_texts:\n",
        "    create_id = tokenizer.convert_tokens_to_ids(x)\n",
        "    token_ids.append(create_id)\n",
        "\n",
        "input_ids = []\n",
        "\n",
        "for item in token_ids:\n",
        "  count = len(item)\n",
        "  temp = item\n",
        "  while count != sequence_length:\n",
        "    temp.append(0)\n",
        "    count = count + 1\n",
        "  input_ids.append(temp)\n",
        "\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for item in input_ids:\n",
        "    temp = []\n",
        "    for number in item:\n",
        "      if number != 0:\n",
        "        temp.append(1)\n",
        "      else:\n",
        "        temp.append(0)\n",
        "    attention_masks.append(temp)\n",
        "\n",
        "bert_input = torch.tensor(input_ids)\n",
        "bert_mask = torch.tensor(attention_masks)\n",
        "\n",
        "with torch.no_grad():   # using torch.no_grad()\n",
        "    outputs = model(input_ids=bert_input, attention_mask=bert_mask)   # generate the vectors for our batch of 8\n",
        "\n",
        "vectors = outputs.pooler_output\n",
        "reg_list = []\n",
        "for item in vectors:\n",
        "    reg_list.append((item.tolist()))\n",
        "\n",
        "vec = []\n",
        "for item in reg_list:\n",
        "    temp = [item]\n",
        "    vec.append(temp)\n",
        "\n",
        "predictions = []\n",
        "for tess in vec:\n",
        "    # Make predictions using the trained Logistic Regression classifier\n",
        "    predicted_class = classifier.predict(tess)\n",
        "\n",
        "    # store the predictions in its own array\n",
        "    predictions.append(int(predicted_class))\n",
        "\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0WJ3oybQcw8",
        "outputId": "27c6cc66-6503-4704-d8b0-643036b7bc3f"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And there you have it. The classifier has accurately identified that the first tweet is negative - $1$ - and the second tweet is positive - $0$. The user can add as many tweets as he/she desires to the initial list, and you can play around and tinker with the code to suit yourself. ENJOY!\n",
        "\n",
        "P.S. - If you find any gramatically errors in this document, please let me know under the open issue for gramatical errors."
      ],
      "metadata": {
        "id": "38I2SqLRYAJ3"
      }
    }
  ]
}